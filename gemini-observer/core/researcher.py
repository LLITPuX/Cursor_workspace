"""
Researcher - Agentic RAG with Cypher Query Tool.

This module provides tool integration for Gemini/OpenAI to execute
Cypher queries against FalkorDB, enabling agentic RAG capabilities.

Flow:
1. Analyst generates search_query from message
2. Researcher receives search_query
3. Researcher asks LLM to formulate Cypher query
4. Execute query against FalkorDB
5. LLM interprets results and generates response
"""

import asyncio
import json
import logging
from typing import Optional, Dict, Any, List
from dataclasses import dataclass

from core.llm_interface import ProviderResponse
from core.switchboard import Switchboard


@dataclass
class QueryResult:
    """Result of a Cypher query execution."""
    success: bool
    data: List[Dict[str, Any]]
    error: Optional[str] = None
    query: Optional[str] = None


class Researcher:
    """
    Agentic RAG: Allows LLM to query the Knowledge Graph.
    
    Provides `query_graph` tool that:
    1. Takes a natural language question
    2. LLM formulates Cypher query
    3. Executes against FalkorDB
    4. Returns structured results
    """
    
    # Prompt for Cypher query generation
    CYPHER_PROMPT = """Ð¢Ð¸ â€” ÐµÐºÑÐ¿ÐµÑ€Ñ‚ Ð· Cypher (Ð¼Ð¾Ð²Ð° Ð·Ð°Ð¿Ð¸Ñ‚Ñ–Ð² Ð´Ð»Ñ Ð³Ñ€Ð°Ñ„Ð¾Ð²Ð¸Ñ… Ð±Ð°Ð· Ð´Ð°Ð½Ð¸Ñ… FalkorDB/Neo4j).

Ð“Ñ€Ð°Ñ„ GeminiMemory Ð¼Ð°Ñ” Ñ‚Ð°ÐºÑƒ ÑÑ…ÐµÐ¼Ñƒ:
- (:User {telegram_id, name}) â€” ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ñ–
- (:Agent {telegram_id, name}) â€” Ð±Ð¾Ñ‚Ð¸
- (:Chat {chat_id, name}) â€” Ñ‡Ð°Ñ‚Ð¸
- (:Message {uid, text, created_at, name}) â€” Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½Ð½Ñ
- (:Thought {topic, new_facts, target_user}) â€” Ð°Ð½Ð°Ð»Ñ–Ñ‚Ð¸Ñ‡Ð½Ñ– Ð²ÑƒÐ·Ð»Ð¸
- (:Day {date}) â€” Ð´Ð½Ñ–
- (:SystemEvent {type, source, details}) â€” ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ– Ð¿Ð¾Ð´Ñ–Ñ—

Ð—Ð²'ÑÐ·ÐºÐ¸:
- [:AUTHORED] â€” Userâ†’Message
- [:GENERATED] â€” Agentâ†’Message  
- [:HAPPENED_IN] â€” Messageâ†’Chat
- [:HAPPENED_AT] â€” Messageâ†’Day
- [:NEXT] â€” Messageâ†’Message (Ñ…Ñ€Ð¾Ð½Ð¾Ð»Ð¾Ð³Ñ–Ñ)
- [:DERIVED_FROM] â€” Thoughtâ†’Message

Ð¡Ñ„Ð¾Ñ€Ð¼ÑƒÐ¹ Cypher-Ð·Ð°Ð¿Ð¸Ñ‚ Ð´Ð»Ñ Ð½Ð°ÑÑ‚ÑƒÐ¿Ð½Ð¾Ð³Ð¾ Ð¿Ð¸Ñ‚Ð°Ð½Ð½Ñ.
Ð’ÐÐ–Ð›Ð˜Ð’Ðž:
1. Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ¹ SEARCH (CONTAINS) Ð¿Ð¾ ÐºÐ»ÑŽÑ‡Ð¾Ð²Ð¸Ð¼ ÑÐ»Ð¾Ð²Ð°Ð¼, Ð° Ð½Ðµ Ñ†Ñ–Ð»Ð¸Ð¼ Ñ„Ñ€Ð°Ð·Ð°Ð¼.
2. Ð¯ÐºÑ‰Ð¾ Ð·Ð°Ð¿Ð¸Ñ‚ Ð¾Ð´Ð½Ñ–Ñ”ÑŽ Ð¼Ð¾Ð²Ð¾ÑŽ, Ð° Ð² Ð±Ð°Ð·Ñ– Ð¼Ð¾Ð¶Ðµ Ð±ÑƒÑ‚Ð¸ Ñ–Ð½ÑˆÐ° â€” ÑˆÑƒÐºÐ°Ð¹ Ð¾Ð±Ð¾Ð¼Ð° Ð¼Ð¾Ð²Ð°Ð¼Ð¸ (Ð£ÐºÑ€/Ð Ð¾Ñ).
   ÐÐ°Ð¿Ñ€Ð¸ÐºÐ»Ð°Ð´: WHERE toLower(m.text) CONTAINS 'Ñ—Ð´Ñƒ' OR toLower(m.text) CONTAINS 'ÐµÐ´Ñƒ'
3. ÐŸÐ¾Ð²ÐµÑ€Ð½Ð¸ Ð¢Ð†Ð›Ð¬ÐšÐ˜ Ð·Ð°Ð¿Ð¸Ñ‚ Ð±ÐµÐ· Ð¿Ð¾ÑÑÐ½ÐµÐ½ÑŒ.
4. Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ¹ LIMIT 10.

ÐŸÐ¸Ñ‚Ð°Ð½Ð½Ñ: """

    # Prompt for interpreting query results
    INTERPRET_PROMPT = """Ð¢Ð¸ Ð¾Ñ‚Ñ€Ð¸Ð¼Ð°Ð² Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¸ Ð·Ð°Ð¿Ð¸Ñ‚Ñƒ Ð´Ð¾ Ð“Ñ€Ð°Ñ„Ð° Ð—Ð½Ð°Ð½ÑŒ. 
Ð†Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚ÑƒÐ¹ Ñ—Ñ… Ñ‚Ð° Ð´Ð°Ð¹ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÑƒ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ñƒ.

Ð—Ð°Ð¿Ð¸Ñ‚: {query}
Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¸: {results}

Ð’Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ (ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾, Ð¿Ð¾ ÑÑƒÑ‚Ñ–):"""

    def __init__(
        self,
        switchboard: Switchboard,
        memory_provider,  # FalkorDBProvider
    ):
        self.switchboard = switchboard
        self.memory = memory_provider
        logging.info("Researcher initialized")

    def _clean_data(self, data: Any) -> Any:
        """Recursively decode bytes and handle graph objects."""
        if isinstance(data, bytes):
            return data.decode('utf-8')
        elif isinstance(data, list):
            return [self._clean_data(item) for item in data]
        elif isinstance(data, dict):
            return {self._clean_data(k): self._clean_data(v) for k, v in data.items()}
        elif hasattr(data, 'properties') and hasattr(data, 'labels'): # Node object
            return {
                "id": getattr(data, 'id', None),
                "labels": getattr(data, 'labels', []),
                "properties": self._clean_data(getattr(data, 'properties', {}))
            }
        elif hasattr(data, 'properties') and hasattr(data, 'relation'): # Edge object
            return {
                "id": getattr(data, 'id', None),
                "type": getattr(data, 'relation', None),
                "properties": self._clean_data(getattr(data, 'properties', {}))
            }
        return data

    async def query_knowledge(self, question: str) -> str:
        """
        Main entry point: Answer a question using the Knowledge Graph.
        
        Args:
            question: Natural language question
            
        Returns:
            Answer string based on graph data
        """
        logging.info(f"ðŸ” Researcher: Processing question: {question[:50]}...")
        
        # Step 1: Generate Cypher query
        cypher_query = await self._generate_cypher(question)
        
        if not cypher_query:
            return "ÐÐµ Ð²Ð´Ð°Ð»Ð¾ÑÑ ÑÑ„Ð¾Ñ€Ð¼ÑƒÐ²Ð°Ñ‚Ð¸ Ð·Ð°Ð¿Ð¸Ñ‚ Ð´Ð¾ Ð±Ð°Ð·Ð¸ Ð·Ð½Ð°Ð½ÑŒ."
        
        logging.info(f"ðŸ” Researcher: Generated Cypher: {cypher_query[:100]}...")
        
        # Step 2: Execute query
        result = await self._execute_query(cypher_query)
        
        if not result.success:
            logging.warning(f"Researcher: Query failed: {result.error}")
            return f"ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð·Ð°Ð¿Ð¸Ñ‚Ñƒ: {result.error}"
        
        if not result.data:
            return "Ð’ Ð±Ð°Ð·Ñ– Ð·Ð½Ð°Ð½ÑŒ Ð½ÐµÐ¼Ð°Ñ” Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–Ñ— Ð·Ð° Ñ†Ð¸Ð¼ Ð·Ð°Ð¿Ð¸Ñ‚Ð¾Ð¼."
        
        logging.info(f"ðŸ” Researcher: Got {len(result.data)} results")
        
        # Step 3: Interpret results
        answer = await self._interpret_results(cypher_query, result.data)
        
        return answer

    async def _generate_cypher(self, question: str) -> Optional[str]:
        """Generate Cypher query from natural language question."""
        prompt = self.CYPHER_PROMPT + question
        
        try:
            response: ProviderResponse = await self.switchboard.generate(
                history=[{"role": "user", "content": prompt}],
                system_prompt=None,
                use_fast=False  # Use Gemini/OpenAI for better query quality
            )
            
            # Clean response
            query = response.content.strip()
            
            # Remove markdown code blocks if present
            if query.startswith("```"):
                lines = query.split("\n")
                query = "\n".join(lines[1:-1] if lines[-1] == "```" else lines[1:])
            
            # Basic validation
            if not any(kw in query.upper() for kw in ["MATCH", "RETURN", "CREATE"]):
                logging.warning(f"Researcher: Invalid Cypher generated: {query}")
                return None
            
            return query.strip()
            
        except Exception as e:
            logging.error(f"Researcher: Failed to generate Cypher: {e}")
            return None

    async def _execute_query(self, cypher_query: str) -> QueryResult:
        """Execute Cypher query against FalkorDB."""
        if not hasattr(self.memory, '_query'):
            return QueryResult(
                success=False,
                data=[],
                error="Memory provider doesn't support _query"
            )
        
        try:
            result = await self.memory._query(cypher_query)
            
            # Parse FalkorDB result format
            # result[0] = headers, result[1] = rows, result[2] = stats
            if not result or len(result) < 2:
                return QueryResult(success=True, data=[], query=cypher_query)
            
            headers = result[0] if result[0] else []
            rows = result[1] if len(result) > 1 else []
            
            # Convert to list of dicts
            # Convert to list of dicts
            data = []
            for row in rows:
                row_dict = {}
                for i, value in enumerate(row):
                    key = headers[i] if i < len(headers) else f"col{i}"
                    # Clean key
                    if isinstance(key, bytes):
                        key = key.decode('utf-8')
                    
                    # Clean value recursively
                    row_dict[key] = self._clean_data(value)
                    
                data.append(row_dict)
            
            return QueryResult(success=True, data=data, query=cypher_query)
            
        except Exception as e:
            logging.error(f"Researcher: Query execution failed: {e}")
            return QueryResult(
                success=False,
                data=[],
                error=str(e),
                query=cypher_query
            )

    async def _interpret_results(
        self, 
        query: str, 
        results: List[Dict[str, Any]]
    ) -> str:
        """Interpret query results and generate human-readable answer."""
        # Limit results for prompt
        results_str = json.dumps(results[:5], ensure_ascii=False, indent=2)
        
        prompt = self.INTERPRET_PROMPT.format(
            query=query,
            results=results_str
        )
        
        try:
            response: ProviderResponse = await self.switchboard.generate(
                history=[{"role": "user", "content": prompt}],
                system_prompt=None,
                use_fast=True  # Use Gemma for faster interpretation
            )
            
            return response.content.strip()
            
        except Exception as e:
            logging.error(f"Researcher: Failed to interpret results: {e}")
            # Fallback: return raw data summary
            return f"Ð—Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ {len(results)} Ð·Ð°Ð¿Ð¸ÑÑ–Ð² Ñƒ Ð±Ð°Ð·Ñ– Ð·Ð½Ð°Ð½ÑŒ."


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Tool Definition for OpenAI Function Calling
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RESEARCHER_TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "query_knowledge_graph",
            "description": "Query the FalkorDB knowledge graph to find information about users, messages, events, and relationships. Use this when you need to look up historical information or facts about people.",
            "parameters": {
                "type": "object",
                "properties": {
                    "question": {
                        "type": "string",
                        "description": "Natural language question to answer using the knowledge graph"
                    }
                },
                "required": ["question"]
            }
        }
    }
]


class ResearcherToolHandler:
    """
    Handler for processing tool calls from OpenAI/Gemini.
    
    Integrates with chat loop to execute knowledge graph queries
    when LLM requests the query_knowledge_graph tool.
    """
    
    def __init__(self, researcher: Researcher):
        self.researcher = researcher
    
    async def handle_tool_call(
        self, 
        tool_name: str, 
        arguments: Dict[str, Any]
    ) -> str:
        """Handle a tool call from the LLM."""
        if tool_name == "query_knowledge_graph":
            question = arguments.get("question", "")
            return await self.researcher.query_knowledge(question)
        
        return f"Unknown tool: {tool_name}"
    
    def get_tools(self) -> List[Dict]:
        """Get tool definitions for OpenAI function calling."""
        return RESEARCHER_TOOLS
