#!/usr/bin/env python3
"""
–ú–æ–¥—É–ª—å –¥–ª—è —ñ–Ω–≥–µ—Å—Ç—ñ—ó MD-—Ñ–∞–π–ª—ñ–≤ —Å–µ—Å—ñ–π —É FalkorDB.
–û–±—Ä–æ–±–ª—è—î —Ñ–∞–π–ª–∏, –≤–∏—Ç—è–≥—É—î —Å—É—Ç–Ω–æ—Å—Ç—ñ —á–µ—Ä–µ–∑ QPE API —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞—î –≤ –≥—Ä–∞—Ñ –∑ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–∏–º–∏ –º–µ—Ç–∫–∞–º–∏.
"""

import os
import re
import sys
import uuid
import json
import httpx
from pathlib import Path
from typing import Optional, List, Dict, Any
from datetime import datetime

try:
    from falkordb import FalkorDB
except ImportError:
    print("–ü–æ–º–∏–ª–∫–∞: –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ falkordb. –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å —á–µ—Ä–µ–∑:")
    print("  pip install falkordb")
    sys.exit(1)


def parse_datetime(date_str: str) -> Dict[str, Optional[str]]:
    """
    –ü–∞—Ä—Å–∏—Ç—å —Ä—è–¥–æ–∫ –¥–∞—Ç–∏ —Ç–∞ —á–∞—Å—É, –ø–æ–≤–µ—Ä—Ç–∞—î date —Ç–∞ time –æ–∫—Ä–µ–º–æ.
    
    –ü—ñ–¥—Ç—Ä–∏–º—É—î —Ñ–æ—Ä–º–∞—Ç–∏:
    - "20 —Å—ñ—á–Ω—è 2026, 14:30:00" (–Ω–æ–≤–∏–π —Ñ–æ—Ä–º–∞—Ç –∑ —á–∞—Å–æ–º)
    - "20 —Å—ñ—á–Ω—è 2026" (—Å—Ç–∞—Ä–∏–π —Ñ–æ—Ä–º–∞—Ç –±–µ–∑ —á–∞—Å—É)
    
    Args:
        date_str: –†—è–¥–æ–∫ –∑ –¥–∞—Ç–æ—é —Ç–∞ –æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ —á–∞—Å–æ–º
        
    Returns:
        –°–ª–æ–≤–Ω–∏–∫ –∑ 'date' —Ç–∞ 'time' (time –º–æ–∂–µ –±—É—Ç–∏ None)
    """
    date_str = date_str.strip()
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ —î —á–∞—Å —É —Ñ–æ—Ä–º–∞—Ç—ñ ", HH:MM:SS"
    time_match = re.search(r',\s*(\d{1,2}:\d{2}:\d{2})$', date_str)
    
    if time_match:
        # –Ñ —á–∞—Å - —Ä–æ–∑–¥—ñ–ª–∏—Ç–∏ –¥–∞—Ç—É —Ç–∞ —á–∞—Å
        time_str = time_match.group(1)
        date_only = date_str[:time_match.start()].strip()
        return {
            'date': date_only,
            'time': time_str
        }
    else:
        # –ù–µ–º–∞—î —á–∞—Å—É - —Ç—ñ–ª—å–∫–∏ –¥–∞—Ç–∞
        return {
            'date': date_str,
            'time': None
        }


def parse_session_file(file_path: str) -> Dict[str, Any]:
    """
    –ü–∞—Ä—Å–∏—Ç—å MD-—Ñ–∞–π–ª —Å–µ—Å—ñ—ó —Ç–∞ –≤–∏—Ç—è–≥—É—î –º–µ—Ç–∞–¥–∞–Ω—ñ —Ç–∞ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è.
    
    Args:
        file_path: –®–ª—è—Ö –¥–æ MD-—Ñ–∞–π–ª—É
        
    Returns:
        –°–ª–æ–≤–Ω–∏–∫ –∑ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏ —Ç–∞ —Å–ø–∏—Å–∫–æ–º –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # –í–∏—Ç—è–≥–Ω—É—Ç–∏ –º–µ—Ç–∞–¥–∞–Ω—ñ –∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    metadata = {}
    header_match = re.search(r'^#\s*–°–µ—Å—ñ—è:\s*(.+?)$', content, re.MULTILINE)
    if header_match:
        metadata['title'] = header_match.group(1).strip()
    
    # –í–∏—Ç—è–≥–Ω—É—Ç–∏ –¥–∞—Ç—É —Ç–∞ —á–∞—Å
    date_match = re.search(r'\*\*–î–∞—Ç–∞:\*\*\s*(.+?)$', content, re.MULTILINE)
    if date_match:
        date_time_str = date_match.group(1).strip()
        # –ü–∞—Ä—Å–∏—Ç–∏ –¥–∞—Ç—É —Ç–∞ —á–∞—Å
        parsed_dt = parse_datetime(date_time_str)
        metadata['date'] = parsed_dt['date']
        metadata['time'] = parsed_dt['time']
    else:
        metadata['date'] = ''
        metadata['time'] = None
    
    # –í–∏—Ç—è–≥–Ω—É—Ç–∏ —Ç–µ–º—É
    topic_match = re.search(r'\*\*–¢–µ–º–∞:\*\*\s*(.+?)$', content, re.MULTILINE)
    if topic_match:
        metadata['topic'] = topic_match.group(1).strip()
    
    # –†–æ–∑–±–∏—Ç–∏ –Ω–∞ –±–ª–æ–∫–∏ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å
    messages = []
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ—à—É–∫—É –±–ª–æ–∫—ñ–≤ "–ó–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞" —Ç–∞ "–í—ñ–¥–ø–æ–≤—ñ–¥—å"
    user_pattern = r'##\s*–ó–∞–ø–∏—Ç\s+–∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞\s+#\d+\s*\n\n(.*?)(?=\n###|\n##|$)'
    assistant_pattern = r'###\s*(?:–ê–Ω–∞–ª—ñ–∑\s+—Ç–∞\s+–¥—ñ—ó|–í—ñ–¥–ø–æ–≤—ñ–¥—å)\s*#\d+\s*\n\n(.*?)(?=\n##|$)'
    
    user_matches = list(re.finditer(user_pattern, content, re.DOTALL | re.IGNORECASE))
    assistant_matches = list(re.finditer(assistant_pattern, content, re.DOTALL | re.IGNORECASE))
    
    # –û–±'—î–¥–Ω–∞—Ç–∏ —Ç–∞ –≤—ñ–¥—Å–æ—Ä—Ç—É–≤–∞—Ç–∏ –∑–∞ –ø–æ–∑–∏—Ü—ñ—î—é –≤ —Ñ–∞–π–ª—ñ
    all_matches = []
    for match in user_matches:
        all_matches.append(('user', match.start(), match.group(1).strip()))
    for match in assistant_matches:
        all_matches.append(('assistant', match.start(), match.group(1).strip()))
    
    all_matches.sort(key=lambda x: x[1])
    
    for role, _, text in all_matches:
        if text.strip():
            messages.append({
                'role': role,
                'content': text.strip()
            })
    
    return {
        'metadata': metadata,
        'messages': messages,
        'file_path': file_path
    }


async def process_message_with_qpe(
    message: Dict[str, Any],
    qpe_url: str
) -> Dict[str, Any]:
    """
    –û–±—Ä–æ–±–ª—è—î –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è —á–µ—Ä–µ–∑ QPE API.
    
    Args:
        message: –°–ª–æ–≤–Ω–∏–∫ –∑ 'role' —Ç–∞ 'content'
        qpe_url: URL QPE Service
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–æ–±–∫–∏ –∑ classifications, entities, embeddings
    """
    async with httpx.AsyncClient(timeout=300.0) as client:
        if message['role'] == 'user':
            # –û–±—Ä–æ–±–∫–∞ –∑–∞–ø–∏—Ç—É –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
            response = await client.post(
                f"{qpe_url}/api/v1/qpe/process-query",
                json={"query": message['content']}
            )
            response.raise_for_status()
            data = response.json()
            
            return {
                'classifications': data.get('classifications', {}),
                'entities': data.get('entities', []),
                'embedding': data.get('embedding', [])
            }
        else:
            # –û–±—Ä–æ–±–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –∞—Å–∏—Å—Ç–µ–Ω—Ç–∞
            # –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏ (—è–∫—â–æ —î —Å—Ç—Ä—É–∫—Ç—É—Ä–∞)
            structure = {
                'analysis': '',
                'response': message['content'],
                'questions': ''
            }
            
            response = await client.post(
                f"{qpe_url}/api/v1/qpe/process-assistant-response",
                json={
                    "response": message['content'],
                    "structure": structure
                }
            )
            response.raise_for_status()
            data = response.json()
            
            return {
                'classifications': data.get('classifications', {}),
                'entities': data.get('entities', []),
                'embeddings': data.get('embeddings', {})
            }


def get_current_timestamp() -> str:
    """–ü–æ–≤–µ—Ä—Ç–∞—î –ø–æ—Ç–æ—á–Ω–∏–π timestamp –≤ ISO —Ñ–æ—Ä–º–∞—Ç—ñ –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≤ Cypher."""
    return datetime.now().isoformat()


def create_session_node(
    graph,
    session_id: str,
    metadata: Dict[str, Any],
    file_path: str
) -> None:
    """–°—Ç–≤–æ—Ä—é—î –≤—É–∑–æ–ª Session –∑ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–∏–º–∏ –º–µ—Ç–∫–∞–º–∏."""
    topic = metadata.get('topic', metadata.get('title', 'Unknown'))
    date = metadata.get('date', '')
    time = metadata.get('time', None)
    
    # –§–æ—Ä–º—É–≤–∞—Ç–∏ –ø–æ–≤–Ω—É –¥–∞—Ç—É –∑ —á–∞—Å–æ–º –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –≥—Ä–∞—Ñ
    # –Ø–∫—â–æ —á–∞—Å —î, –¥–æ–¥–∞—î–º–æ –π–æ–≥–æ –¥–æ –¥–∞—Ç–∏
    date_time_str = date
    if time:
        date_time_str = f"{date}, {time}"
    
    timestamp = get_current_timestamp()
    query = """
    CREATE (s:Session {
        id: $session_id,
        topic: $topic,
        file_path: $file_path,
        date: $date,
        time: $time,
        date_time: $date_time,
        created_at: $timestamp,
        valid_from: $timestamp,
        valid_to: null
    })
    RETURN s
    """
    
    graph.query(
        query,
        {
            'session_id': session_id,
            'topic': topic,
            'file_path': file_path,
            'date': date,
            'time': time if time else '',
            'date_time': date_time_str,
            'timestamp': timestamp
        }
    )


def create_message_node(
    graph,
    message_id: str,
    session_id: str,
    role: str,
    content: str,
    prev_message_id: Optional[str] = None
) -> None:
    """–°—Ç–≤–æ—Ä—é—î –≤—É–∑–æ–ª Message —Ç–∞ –∑–≤'—è–∑–∫–∏ –∑ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–∏–º–∏ –º–µ—Ç–∫–∞–º–∏."""
    timestamp = get_current_timestamp()
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—É–∑–ª–∞ Message
    query = """
    MATCH (s:Session {id: $session_id})
    CREATE (m:Message {
        id: $message_id,
        role: $role,
        content: $content,
        created_at: $timestamp,
        valid_from: $timestamp,
        valid_to: null
    })
    CREATE (s)-[:HAS_MESSAGE {
        created_at: $timestamp,
        valid_from: $timestamp,
        valid_to: null
    }]->(m)
    RETURN m
    """
    
    graph.query(
        query,
        {
            'session_id': session_id,
            'message_id': message_id,
            'role': role,
            'content': content,
            'timestamp': timestamp
        }
    )
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑–≤'—è–∑–∫—É NEXT (—è–∫—â–æ —î –ø–æ–ø–µ—Ä–µ–¥–Ω—î –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è)
    if prev_message_id:
        query = """
        MATCH (prev:Message {id: $prev_id}), (curr:Message {id: $curr_id})
        CREATE (prev)-[:NEXT {
            created_at: $timestamp,
            valid_from: $timestamp,
            valid_to: null
        }]->(curr)
        RETURN prev, curr
        """
        
        graph.query(
            query,
            {
                'prev_id': prev_message_id,
                'curr_id': message_id,
                'timestamp': timestamp
            }
        )


def create_entity_nodes_and_links(
    graph,
    message_id: str,
    entities: List[Dict[str, Any]],
    entity_embeddings: Dict[str, List[float]]
) -> None:
    """
    –°—Ç–≤–æ—Ä—é—î –≤—É–∑–ª–∏ Entity —Ç–∞ –∑–≤'—è–∑–∫–∏ [:MENTIONS] –∑ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–∏–º–∏ –º–µ—Ç–∫–∞–º–∏.
    
    Args:
        graph: FalkorDB –≥—Ä–∞—Ñ
        message_id: ID –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è
        entities: –°–ø–∏—Å–æ–∫ —Å—É—Ç–Ω–æ—Å—Ç–µ–π –∑ QPE
        entity_embeddings: –°–ª–æ–≤–Ω–∏–∫ {entity_name: embedding_vector}
    """
    for entity in entities:
        entity_name = entity.get('text', '').strip()
        entity_type = entity.get('type', 'Unknown')
        
        if not entity_name:
            continue
        
        # –û—Ç—Ä–∏–º–∞—Ç–∏ embedding –¥–ª—è —Ü—ñ—î—ó —Å—É—Ç–Ω–æ—Å—Ç—ñ
        embedding = entity_embeddings.get(entity_name, None)
        
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ –∞–±–æ –æ–Ω–æ–≤–∏—Ç–∏ Entity
        entity_id = str(uuid.uuid4())
        timestamp = get_current_timestamp()
        
        if embedding:
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ embedding —è–∫ JSON —Ä—è–¥–æ–∫ (FalkorDB –º–æ–∂–µ –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞—Ç–∏ vecf32 –Ω–∞–ø—Ä—è–º—É)
            embedding_json = json.dumps(embedding)
            
            query = """
            MERGE (e:Entity {name: $entity_name})
            ON CREATE SET 
                e.id = $entity_id,
                e.type = $entity_type,
                e.embedding = $embedding,
                e.created_at = $timestamp,
                e.valid_from = $timestamp,
                e.valid_to = null
            ON MATCH SET
                e.valid_to = null
            WITH e
            MATCH (m:Message {id: $message_id})
            CREATE (m)-[:MENTIONS {
                weight: 1.0,
                created_at: $timestamp,
                valid_from: $timestamp,
                valid_to: null
            }]->(e)
            RETURN e, m
            """
            
            graph.query(
                query,
                {
                    'entity_name': entity_name,
                    'entity_id': entity_id,
                    'entity_type': entity_type,
                    'embedding': embedding_json,
                    'message_id': message_id,
                    'timestamp': timestamp
                }
            )
        else:
            # –Ø–∫—â–æ –Ω–µ–º–∞—î embedding, —Å—Ç–≤–æ—Ä—é—î–º–æ –±–µ–∑ –Ω—å–æ–≥–æ
            query = """
            MERGE (e:Entity {name: $entity_name})
            ON CREATE SET 
                e.id = $entity_id,
                e.type = $entity_type,
                e.created_at = $timestamp,
                e.valid_from = $timestamp,
                e.valid_to = null
            ON MATCH SET
                e.valid_to = null
            WITH e
            MATCH (m:Message {id: $message_id})
            CREATE (m)-[:MENTIONS {
                weight: 1.0,
                created_at: $timestamp,
                valid_from: $timestamp,
                valid_to: null
            }]->(e)
            RETURN e, m
            """
            
            graph.query(
                query,
                {
                    'entity_name': entity_name,
                    'entity_id': entity_id,
                    'entity_type': entity_type,
                    'message_id': message_id,
                    'timestamp': timestamp
                }
            )


def ensure_vector_index(graph) -> None:
    """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —Ç–∞ —Å—Ç–≤–æ—Ä—é—î –≤–µ–∫—Ç–æ—Ä–Ω–∏–π —ñ–Ω–¥–µ–∫—Å –¥–ª—è Entity, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ."""
    # –ü—Ä–∏–º—ñ—Ç–∫–∞: FalkorDB –º–æ–∂–µ –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞—Ç–∏ –≤–µ–∫—Ç–æ—Ä–Ω—ñ —ñ–Ω–¥–µ–∫—Å–∏ –Ω–∞–ø—Ä—è–º—É
    # –Ø–∫—â–æ –ø—ñ–¥—Ç—Ä–∏–º—É—î, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ, —ñ–Ω–∞–∫—à–µ - –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ
    try:
        query = """
        CREATE VECTOR INDEX FOR (e:Entity) ON (e.embedding) 
        OPTIONS {dimension: 768, similarityFunction: 'cosine'}
        """
        graph.query(query)
        print("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∏–π —ñ–Ω–¥–µ–∫—Å —Å—Ç–≤–æ—Ä–µ–Ω–æ –∞–±–æ –≤–∂–µ —ñ—Å–Ω—É—î")
    except Exception as e:
        # –Ü–Ω–¥–µ–∫—Å –º–æ–∂–µ –≤–∂–µ —ñ—Å–Ω—É–≤–∞—Ç–∏ –∞–±–æ –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞—Ç–∏—Å—è
        error_str = str(e).lower()
        if "already exists" in error_str or "not supported" in error_str or "syntax" in error_str:
            print(f"‚ÑπÔ∏è  –í–µ–∫—Ç–æ—Ä–Ω–∏–π —ñ–Ω–¥–µ–∫—Å –Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω–æ (–º–æ–∂–µ –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞—Ç–∏—Å—è): {e}")
        else:
            print(f"‚ö†Ô∏è  –ü–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è –ø—Ä–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ —ñ–Ω–¥–µ–∫—Å—É: {e}")


async def ingest_session_file(
    file_path: str,
    graph_name: str = os.getenv("FALKORDB_GRAPH_NAME", "agent_memory"),
    falkordb_host: str = "localhost",
    falkordb_port: int = 6379,
    qpe_url: str = "http://localhost:8001"
) -> None:
    """
    –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è —ñ–Ω–≥–µ—Å—Ç—ñ—ó —Å–µ—Å—ñ—ó.
    
    Args:
        file_path: –®–ª—è—Ö –¥–æ MD-—Ñ–∞–π–ª—É —Å–µ—Å—ñ—ó
        graph_name: –ù–∞–∑–≤–∞ –≥—Ä–∞—Ñ—É –≤ FalkorDB
        falkordb_host: –•–æ—Å—Ç FalkorDB
        falkordb_port: –ü–æ—Ä—Ç FalkorDB
        qpe_url: URL QPE Service
    """
    print(f"üìñ –ß–∏—Ç–∞–Ω–Ω—è —Ñ–∞–π–ª—É: {file_path}")
    
    # –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª—É
    parsed = parse_session_file(file_path)
    print(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(parsed['messages'])} –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å")
    
    # –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ FalkorDB
    print(f"üîå –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ FalkorDB –Ω–∞ {falkordb_host}:{falkordb_port}...")
    client = FalkorDB(host=falkordb_host, port=falkordb_port, password=None)
    graph = client.select_graph(graph_name)
    print(f"‚úÖ –ü—ñ–¥–∫–ª—é—á–µ–Ω–æ –¥–æ –≥—Ä–∞—Ñ—É '{graph_name}'")
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —ñ–Ω–¥–µ–∫—Å—É
    ensure_vector_index(graph)
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è Session
    session_id = str(uuid.uuid4())
    print(f"üìù –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å–µ—Å—ñ—ó: {session_id}")
    create_session_node(graph, session_id, parsed['metadata'], file_path)
    
    # –û–±—Ä–æ–±–∫–∞ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å
    prev_message_id = None
    
    for i, message in enumerate(parsed['messages'], 1):
        print(f"  üì® –û–±—Ä–æ–±–∫–∞ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è {i}/{len(parsed['messages'])} ({message['role']})...")
        
        # –û–±—Ä–æ–±–∫–∞ —á–µ—Ä–µ–∑ QPE
        qpe_result = await process_message_with_qpe(message, qpe_url)
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è Message
        message_id = str(uuid.uuid4())
        create_message_node(
            graph,
            message_id,
            session_id,
            message['role'],
            message['content'],
            prev_message_id
        )
        
        # –û–±—Ä–æ–±–∫–∞ —Å—É—Ç–Ω–æ—Å—Ç–µ–π
        entities = []
        entity_embeddings = {}
        
        if message['role'] == 'user':
            # –î–ª—è user messages entities –≤–∂–µ —î –≤ qpe_result
            entities = qpe_result.get('entities', [])
            if entities:
                print(f"    üîç –ó–Ω–∞–π–¥–µ–Ω–æ {len(entities)} —Å—É—Ç–Ω–æ—Å—Ç–µ–π")
                # –î–ª—è user messages embedding –≤–∂–µ —î –≤ qpe_result
                embedding = qpe_result.get('embedding', [])
                # –°—Ç–≤–æ—Ä–∏—Ç–∏ embedding –¥–ª—è –∫–æ–∂–Ω–æ—ó —Å—É—Ç–Ω–æ—Å—Ç—ñ (—Å–ø—Ä–æ—â–µ–Ω–æ - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∑–∞–≥–∞–ª—å–Ω–∏–π)
                for entity in entities:
                    entity_name = entity.get('text', '').strip()
                    if entity_name and embedding:
                        entity_embeddings[entity_name] = embedding
        else:
            # –î–ª—è assistant messages entities –≤ –æ–∫—Ä–µ–º–∏—Ö –ø–æ–ª—è—Ö
            analysis_entities = qpe_result.get('analysis_entities', [])
            response_entities = qpe_result.get('response_entities', [])
            entities = analysis_entities + response_entities
            
            if entities:
                print(f"    üîç –ó–Ω–∞–π–¥–µ–Ω–æ {len(entities)} —Å—É—Ç–Ω–æ—Å—Ç–µ–π")
                # –î–ª—è assistant messages embeddings –≤ —Å–ª–æ–≤–Ω–∏–∫—É
                embeddings = qpe_result.get('embeddings', {})
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ embedding –∑ response —á–∞—Å—Ç–∏–Ω–∏
                response_embedding = embeddings.get('response', [])
                for entity in entities:
                    entity_name = entity.get('text', '').strip()
                    if entity_name and response_embedding:
                        entity_embeddings[entity_name] = response_embedding
        
        if entities:
            
            # –°—Ç–≤–æ—Ä–∏—Ç–∏ Entity —Ç–∞ –∑–≤'—è–∑–∫–∏
            create_entity_nodes_and_links(
                graph,
                message_id,
                entities,
                entity_embeddings
            )
        
        prev_message_id = message_id
    
    print(f"‚úÖ –Ü–Ω–≥–µ—Å—Ç—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –°–µ—Å—ñ—è –∑–±–µ—Ä–µ–∂–µ–Ω–∞ –∑ ID: {session_id}")
