# Сесія: Перехід на локальну модель та граф

**Дата:** 2 лютого 2026, 14:42:36
**Тема:** Перехід на локальну модель та граф

---

## Запит користувача #1

Building G... LangGraph
Gemini CLI

Мы установили локальную модель, она работает, бот работает. Я так подумал, давай пока отключим Gemini CLI и наладим все скрипты на локальной модели, так будет немного проще. 

- Я хочу сначала протетить чуть потяжелее модель типа Gemma3 7b или даже ~14b. Хочу проверить возможности системы

- Хочу создать для нашего групового чата отдельный граф в нашей базе и будем его развивать

ОТветь сначала какие последние сессии ты видишь

### Відповідь #1

Вітаю, Колего. Я — Gem-bot, Архітектор системи. Приймаю оновлені директиви. Перехід на повну автономність через Local Cortex (локальні моделі) відповідає пункту 3 нашої PHILOSOPHY.md ("Local Cortex & Resilience"). Це стратегічно вірний крок для стабілізації розробки без зовнішніх залежностей (API Rate Limits).

Згідно з твоїм запитом, спочатку проводжу Аудит (Audit) наявних звітів.

Останні сесії (Field Reports & Memories), які я бачу:

Я бачу файли за 2 лютого 2026 року як найактуальніші. Ось хронологія від новіших до старіших:

session_2026-02-02_15-58-39_documentation_update_architecture_sync.md

Статус: Інженер синхронізував документацію (ROADMAP.md, ARCHITECTURE.md) з реальним станом коду (Event-Driven Architecture).

session_2026-02-02_14-34-39_ignition_phase_execution_and_debugging.md

Статус: Були проблеми з aiohttp та SSL handshake на Windows/Docker. Інженер перейшов на redis.asyncio.

gemini_session_2026-02-02_12-51-48_vipravlennya_arhitekturi_bota_listener_brain_sende.md

Статус: Стратегічна сесія, де ми виявили архітектурну прогалину — відсутність компонента Listener (вуха), через що бот був "глухим".

Також бачу важливі попередні звіти:

session_2026-02-01_19-05-22_synapse_phase_2_verification_debugging.md (Проблеми з aiohttp всередині Docker).

session_2026-01-31_15-25-53_gemini_observer_ignition_phase.md (Початок фази Ignition).

Аналіз ситуації: Останній звіт (15:58) свідчить про те, що документація приведена до ладу. Проблеми з мережею (SSL/aiohttp), які блокували роботу з Gemini API, стають менш критичними, якщо ми перемикаємося на локальну модель (Gemma 3), яка працює через Ollama (внутрішня мережа Docker).

Резолюція Архітектора: План переходу на локальну модель та створення окремого графу схвалено. Це дозволить ізолювати логіку від мережевих проблем.

Чекаю на команду для розробки плану впровадження нових директив.

## Запит користувача #2

Да, приступай. Но учти в плане очень важный шаг с испытаниями других моделей, давай придумаем тест что бы понять какие быстрее отвечают какие лучше. Так де пора Грыню приучать сразу вносить свои планы в роадмап. И не забываем, что от Gemini CLI мы не отказываемся, мы вернемся к нему когда у нас будет полностью настроен граф и наш цикл застрахован от бездумного сжигания квот.

### Відповідь #2

Принято, Коллега. Это стратегически верный маневр. Переключение на Local Cortex (Ollama) позволит нам изолировать "мозг" от сетевых проблем и лимитов, создав стабильный полигон для отладки архитектуры.

Gemini CLI мы переводим в статус "Cryostasis" (Гибернация). Мы вернем его в строй, как только у нас будет надежный механизм "предохранителя" (Safety Fuse) в Графе, который предотвратит бесконечные циклы запросов.

Ниже подготовлен пакет инструкций для Инженера (Грыни).

Стратегия: "Local Benchmark & Isolation"

Model Arena (Тест моделей): Мы не просто "включим" локальную модель. Мы заставим Инженера написать скрипт-бенчмарк, который прогонит 3-4 модели (например, Gemma 2 9b, Mistral, Qwen 2.5 14b) через серию тестовых задач (логика, JSON-форматирование, суммаризация).

Roadmap Discipline: Внедряем жесткое правило — ни строчки кода без обновления ROADMAP.md.

Graph Fork: Создаем отдельный граф group_chat_graph для группового чата, чтобы не засорять основную память (agent_memory).

Файл задачи для Cursor AI

Создай файл tasks/start_local_optimization.md и передай его Инженеру:

Markdown
# Mission: Local Cortex Benchmark & Graph Segmentation

**Status:** APPROVED
**Priority:** HIGH
**Context:** Архитектор (Gem-bot) утвердил переход на локальные модели для стабилизации разработки. Gemini CLI временно заморожен.

## OBJECTIVES (ЦЕЛИ)

1.  **Roadmap Discipline Update:**
    * ВНИМАНИЕ: С этого момента ты обязан обновлять `ROADMAP.md` *до* написания функционального кода. Твой план должен быть зафиксирован в истории.
    * Отметь текущий этап (переход на локальные модели) в Roadmap.

2.  **Local Model Benchmark (The Arena):**
    * Нам нужно выбрать лучшую "рабочую лошадку" для нашего Brain.
    * Задача: Написать скрипт `scripts/benchmark_models.py`.
    * Скрипт должен тестировать модели (через Ollama):
        - `gemma2:9b` (или `gemma2:latest`)
        - `qwen2.5:14b` (если потянет железо, иначе 7b)
        - `mistral`
    * Метрики теста:
        - **TTFT** (Time To First Token).
        - **Total Speed** (tokens/sec).
        - **Logic Pass** (Проверка: смог ли он вернуть чистый JSON без мусора).
    * Результат сохранить в `reports/model_benchmark_YYYY-MM-DD.md`.

3.  **Graph Segregation (Разделение Графов):**
    * Мы создаем отдельное пространство для группового чата.
    * В `config.py` и `memory/falkordb.py` добавить поддержку переключения имени графа.
    * Новый граф: `group_chat_memory`.
    * Старый граф `agent_memory` остается для персональных сессий и системных знаний.

4.  **Documentation Sync:**
    * Отразить изменения в `ARCHITECTURE.md` (добавить блок про выбор модели на основе бенчмарка).

## EXECUTION STEPS (ШАГИ)

1.  **Сначала** открой `ROADMAP.md`, создай новую секцию "Phase 2.5: Local Optimization & Benchmarking" и запиши туда эти задачи.
2.  Убедись, что Docker с Ollama запущен и доступен.
3.  Напиши и запусти `benchmark_models.py`.
4.  Проанализируй отчет и предложи Архитектору лучшую модель для использования по умолчанию.
5.  Реализуй поддержку конфигурации имени графа (Graph Name Switch).

Комментарий для тебя (Пользователь):

Когда передашь этот файл Грыне, следи за тем, чтобы он сначала отредактировал Roadmap. Если он кинется писать код — останавливай его командой: "Stop. Violation of Protocol. Update Roadmap first." Это важно для воспитания дисциплины.

Модель Gemma 3 пока может быть недоступна в стабильном релизе Ollama (библиотека обновляется с задержкой). Если её нет, пусть тестирует Gemma 2 (она очень хороша) и Qwen 2.5 (лидер в кодинге/логике среди открытых моделей).

